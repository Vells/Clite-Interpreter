# coding=utf-8"""CS 364 Programming LanguagesAuthor: Vela Dimitrova MinevaDate: 03/06/2015"""import sysimport reimport osimport tokensclass Lexer(object):    """    A Lexer class that models a Clite lexical analyzer.    The lexer recognizes Clite tokens and token_generator() method    returns them one at a time upon a call of the builtin __next__() function.    """    # Token status indicator    # VALIDITY = {0: "INVALID", 1: "VALID"}    def __init__(self, filename):        self.Clite_file = self.open_file(filename)        self.token_generator()    @staticmethod    def open_file(filename):        """        A method that, given a filename returns the opened file object.        If the file is not successfully opened, a message is printed and        the program is terminated.        :param filename: type(filename) is a string; len(filename) > 0        :return type(file) is a file object        """        if os.path.isfile(filename):            try:                file = open(filename)            except IOError:                sys.stdout.write("Could not open {0}!\n".format(filename))                sys.exit(0)            else:                return file        else:            sys.stdout.write("The file {0} does not exist!\n".format(filename))            sys.exit(0)    @staticmethod    def remove_comments(line):        """        A method that removes comments from a line        and returns it.        :param line: type(line) is a string        :return: type(processed_line) is a string        """        processed_line = re.sub("//.*$", "", line)        return processed_line    @staticmethod    def get_token_regex(collection, patt=""):        """        A method that generates a regular expression by a given        collection. The optional patt parameters allows to        attach a pattern to the front of the regular expression        :param collection: type(collection) is a collection        :param patt: patt is a regex [optional]        :return: generated regular expression, type(expression) is string        """        expression = ""        if patt:            expression += patt + "|"        for token in collection:            expression += "(" + re.escape(token) + ")|"        return expression[0:-1]    def token_generator(self):        """        A method that goes through each line in a file, removes comments,        splits each line on tokens and yields a 4-tuple of the form        (NAME, TOKEN, LINE NUMBER, VALIDITY)        03/28/2015: Method modified to yield a 4-tuple of the form                    (CODE, TOKEN, LINE NUMBER)        The program is terminated if unrecognized token is encountered        """        # Compile regular expression patterns that will be used to identify tokens        int_lit = re.compile(tokens.INTLIT[2])              # an integer literal        real_number = re.compile(tokens.REAL_NUMBER[2])      # a real number        identifier = re.compile(tokens.ID[2])        # identifier        # Form a regular expression        complex_tokens_regex = self.get_token_regex(tokens.COMPLEX_TOKENS, "\s+")        single_tokens_regex = self.get_token_regex(tokens.SINGLE_TOKENS)        token_regex = complex_tokens_regex + "|" + single_tokens_regex        line_number = 0        # Process each line in the file        for number, line in enumerate(self.Clite_file):            line_number = number + 1            # If the line contains a comment, remove the comment            line = self.remove_comments(line)            # Split the processed line on the formed regular expression            line_tokens = re.split(token_regex, line)            # Process each token in a line            for tok in line_tokens:                if not tok:                    continue                elif tok in tokens.COMPLEX_TOKENS.keys():                    code = tokens.COMPLEX_TOKENS[tok][0]                    name = tokens.COMPLEX_TOKENS[tok][1]                    yield (code, name, tok, line_number)                elif real_number.search(tok):                    code = tokens.REAL_NUMBER[0]                    name = tokens.REAL_NUMBER[1]                    yield (code, name, tok, line_number)                elif int_lit.search(tok):                    yield (tokens.INTLIT[0], tokens.INTLIT[1], int(tok), line_number)                elif tok in tokens.KEYWORDS:                    yield (tokens.KEYWORDS[tok], tokens.KEYWORD, tok, line_number)                elif identifier.search(tok):                    yield (tokens.ID[0], tokens.ID[1], tok, line_number)                elif tok in tokens.SINGLE_TOKENS.keys():                    code = tokens.SINGLE_TOKENS[tok][0]                    name = tokens.SINGLE_TOKENS[tok][1]                    yield (code, name, tok, line_number)                else:                    # yield (tokens.UNRECOGNIZED_TOKEN[0], tokens.UNRECOGNIZED_TOKEN[1],                    #       tok, line_number, self.VALIDITY[0])                    sys.exit("ERROR at LINE NUMBER {0}: "                             "Unrecognized token '{1}'!\nGood night!".format(line_number, tok))        while True:            yield (tokens.END_OF_FILE[0], tokens.END_OF_FILE[1], "", line_number)